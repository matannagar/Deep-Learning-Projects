# -*- coding: utf-8 -*-
"""Rock Paper Scissors Image Classification

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10mvYMjeRENJaOUswLg-Z4IurTXVDV2bQ

**Deep Learning Image Classification using Neural Network**
<br> by 
Matan-Ben Nagar
&
Yaara Kresner-Barak

## Importing our libraries for the project
"""

import matplotlib.pyplot as plt
import numpy as np

import tensorflow as tf
import tensorflow_datasets as tfds

from tensorflow import keras
from keras.layers import Dense, Flatten,Conv2D,MaxPool2D,Dropout,AveragePooling2D
tfds.disable_progress_bar()

"""## Load rock_paper_scissors dataset
The dataset contains 2892 images of hands playing rock, paper, scissor game.
Its have two features- image (300, 300, 3) and lable.



"""

##import the dataset from tensorflow_datasets library 
builder = tfds.builder('rock_paper_scissors')

"""Split Rock, Paper, Scissors data
The train set contains 2520 images, and the test set contains 372 images.
"""

#load the train and test sets from the DB 
ds_train = tfds.load(name="rock_paper_scissors", split="train")
ds_test = tfds.load(name="rock_paper_scissors", split="test")

"""Converting the tensorflow dataset format into numpy format,

Create numpy arrays that contains the images and the labels separately,

And change the images three color channels RGB format to one color channel (to reduce the unimportant data)

"""

train_images = np.array([example['image'].numpy()[:,:,0] for example in ds_train])
train_labels = np.array([example['label'].numpy() for example in ds_train])

test_images = np.array([example['image'].numpy()[:,:,0] for example in ds_test])
test_labels = np.array([example['label'].numpy() for example in ds_test])

"""Reshaping the images to 300 x 300 x 1 (add color feature- grayscale images). """

train_images = train_images.reshape(2520, 300, 300, 1)
test_images = test_images.reshape(372, 300, 300, 1)

"""getting us ready to be able to convert it from a scale of 0 to 1
instead of 0 to 255

"""

train_images = train_images.astype('float32')
test_images = test_images.astype('float32')

"""##Normalize the Data
Train images dividing equal by 255,
So the max value we can have is 255 because RGB values are between 0 and 255 so by doing this we're scaling every value to be between 0 & 1 and this is just a good common practice that helps you classify it.
It helps the basically network learn better than if you use the 0 to 255 values you could leave it 0 to 255 but it's just ultimately it's gonna probably decrease your performance a bit, so it's a common step to normalize between 0 & 1.

"""

train_images /= 255
test_images /= 255

"""#Logistic regression

"""

# Output layer.
model_lr = keras.Sequential([
   keras.layers.Flatten(),
   keras.layers.Dense(3, activation='softmax')
])


# adam_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
rmsprop_optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)

model_lr.compile(
    optimizer=rmsprop_optimizer,
    loss=tf.keras.losses.sparse_categorical_crossentropy,
    metrics=['accuracy']
)
model_lr.fit(train_images, train_labels, epochs=5, batch_size=32)

model_lr.evaluate(test_images, test_labels)

"""##MLP

This model network layer transform the 300 by 300 image into single colum,
After that we have two layers of activation relu function- because the constant gradient of ReLUs results in faster learning.
Finally ,the output layer going to be the same size as the number of labels we trying to classify- we use softmax because it efficient in classification problems.
"""

model = keras.Sequential([
  Flatten(),
  Dense(512, activation='relu'),
  Dense(256, activation='relu'),
  Dense(3, activation='softmax')
])

#setup loss function
model.compile(optimizer='adam',
              loss=keras.losses.SparseCategoricalCrossentropy(),
              metrics=['accuracy'])

#fit our data to the model
model.fit(train_images, train_labels, epochs=5, batch_size=32)

model.evaluate(test_images, test_labels)

"""In this case we overfitting to our data - the model not learning the train data. 
we can see it by the results- the accuracy in the train data is 0.89 and the accuracy in the test data is 0.44.

##Convolutional Neural Network
This time the first layer will be Conv2D()  because the dataset consist of 2D images. The first variable inserted in the function is basically how many times a smaller gird is passing over the image <br>
this is how big or smaller grid is so if I said three and I didn't pass in three to start off and we'll leave the rides at one two one that just means they'll move one every time so it's gonna be a sliding window of three by three
"""

pip install -U keras-tuner

# Instructions and exmaples of how to use Keras Tuner can be found in here 
# https://keras.io/keras_tuner/
from kerastuner.tuners import RandomSearch

def build_model(hp):
  model = keras.Sequential()

  model.add(AveragePooling2D(6,3,input_shape=(300,300,1)))

  model.add(Conv2D(64,3,activation='relu'))
  model.add(Conv2D(32,3,activation='relu'))

  model.add(MaxPool2D(2,2))
  model.add(Dropout(0.5))
  model.add(Flatten())
  #hp.Choise function will allow us to test out different variables for this dense layer 
  model.add(Dense(hp.Choice("Dense layer", [64, 128, 256, 512, 1024,2048]), activation='relu'))

  model.add(keras.layers.Dense(3, activation='softmax'))

  model.compile(optimizer='adam',
              loss=keras.losses.SparseCategoricalCrossentropy(),
              metrics=['accuracy'])
  
  return model

# randomSearch will run random combination we insert into the model until it reaches the max_trials or until all of the trials are done.
# the tuner will result in giving an array of models with different combination from the one which performed highest to the lowest
# we can get the models by using tuner.get_best_model()
tuner = RandomSearch(
    # calling the build model function
    build_model, 
    # our objective is the validation accuracy - how well will it do on our test set?
    objective='val_accuracy',
    max_trials=8,
)
# what are we trying to optimise this network for? validation_data
tuner.search(train_images, train_labels, validation_data=(test_images, test_labels), epochs=10, batch_size=32)

tuner.get_best_models()[0].evaluate(test_images,test_labels)
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiGY1xBHaYiU"
      },
      "source": [
        "**Deep Learning Image Classification using Neural Network**\n",
        "<br> by \n",
        "Matan-Ben Nagar\n",
        "&\n",
        "Yaara Kresner-Barak"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTMs24Y1gEEG"
      },
      "source": [
        "## Import Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wTC_pP87gIkG"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras.layers import Dense\n",
        "tfds.disable_progress_bar()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mj-hCwENhrQ-"
      },
      "source": [
        "## Load rock_paper_scissors dataset\n",
        "The dataset contains 2892 images of hands playing rock, paper, scissor game.\n",
        "Its have two features- image (300, 300, 3) and lable.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "id": "2-DCEGhnh_4m"
      },
      "outputs": [],
      "source": [
        "##import the dataset from tensorflow_datasets library \n",
        "builder = tfds.builder('rock_paper_scissors')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxzo48KfiiGH"
      },
      "source": [
        "Split Rock, Paper, Scissors data\n",
        "The train set contains 2520 images, and the test set contains 372 images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNfZOqrcioeH",
        "outputId": "3a23854a-2ba0-414f-c02a-0b18fa43bd46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\matan\\tensorflow_datasets\\rock_paper_scissors\\3.0.0...\u001b[0m\n",
            "\u001b[1mDataset rock_paper_scissors downloaded and prepared to C:\\Users\\matan\\tensorflow_datasets\\rock_paper_scissors\\3.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "#load the train and test sets from the DB \n",
        "ds_train = tfds.load(name=\"rock_paper_scissors\", split=\"train\")\n",
        "ds_test = tfds.load(name=\"rock_paper_scissors\", split=\"test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhRqwjTi-XdZ"
      },
      "source": [
        "Converting the tensorflow dataset format into numpy format,\n",
        "\n",
        "Create numpy arrays that contains the images and the labls separately,\n",
        "\n",
        "And change the images three color channels RGB format to one color channel (to reduce the unimportant data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KOahag-mjuof"
      },
      "outputs": [],
      "source": [
        "train_images = np.array([example['image'].numpy()[:,:,0] for example in ds_train])\n",
        "train_labels = np.array([example['label'].numpy() for example in ds_train])\n",
        "\n",
        "test_images = np.array([example['image'].numpy()[:,:,0] for example in ds_test])\n",
        "test_labels = np.array([example['label'].numpy() for example in ds_test])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnv8_A23JUbS"
      },
      "source": [
        "Reshaping the images to 300 x 300 x 1 (add color feature- grayscale images). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Sg56AOko4qby"
      },
      "outputs": [],
      "source": [
        "train_images = train_images.reshape(2520, 300, 300, 1)\n",
        "test_images = test_images.reshape(372, 300, 300, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLob8ZC5JgOP"
      },
      "source": [
        "getting us ready to be able to convert it from a scale of 0 to 1\n",
        "instead of 0 to 255\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "A7Ra1QAUJhO6"
      },
      "outputs": [],
      "source": [
        "train_images = train_images.astype('float32')\n",
        "test_images = test_images.astype('float32')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trL1RIfDJ0hW"
      },
      "source": [
        "##Normalize the Images\n",
        "Train images dividing equal by 255,\n",
        "So the max value we can have is 255 because RGB values are between 0 and 255 so by doing this we're scaling every value to be between 0 & 1 and this is just a good common practice that helps you classify it.\n",
        "It helps the basically network learn better than if you use the 0 to 255 values you could leave it 0 to 255 but it's just ultimately it's gonna probably decrease your performance a bit, so it's a common step to normalize between 0 & 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VGgO23W14xB9"
      },
      "outputs": [],
      "source": [
        "train_images /= 255\n",
        "test_images /= 255"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWcpsdx4KNap"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "rXLDfJfrKQdU"
      },
      "outputs": [],
      "source": [
        "# Output layer.\n",
        "model_lr = keras.Sequential([\n",
        "   keras.layers.Flatten(),\n",
        "   keras.layers.Dense(3, activation='softmax')\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "8Q10PNAeKttm"
      },
      "outputs": [],
      "source": [
        "# adam_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "rmsprop_optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
        "\n",
        "model_lr.compile(\n",
        "    optimizer=rmsprop_optimizer,\n",
        "    loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vDqYATCK4sK",
        "outputId": "84a2dc86-64b1-45ce-ad58-8e7fe1bb717e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "79/79 [==============================] - 1s 8ms/step - loss: 55.8028 - accuracy: 0.3448\n",
            "Epoch 2/5\n",
            "79/79 [==============================] - 1s 9ms/step - loss: 38.1533 - accuracy: 0.4274\n",
            "Epoch 3/5\n",
            "79/79 [==============================] - 1s 9ms/step - loss: 34.7076 - accuracy: 0.4444\n",
            "Epoch 4/5\n",
            "79/79 [==============================] - 1s 9ms/step - loss: 29.2796 - accuracy: 0.5135\n",
            "Epoch 5/5\n",
            "79/79 [==============================] - 1s 9ms/step - loss: 27.6304 - accuracy: 0.5163\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7b3d3df890>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_lr.fit(train_images, train_labels, epochs=5, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5Jhn90LK5Ic",
        "outputId": "f9bd119f-e446-4cb5-9165-736d0f33d660"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12/12 [==============================] - 0s 7ms/step - loss: 9.2975 - accuracy: 0.5699\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[9.297526359558105, 0.5698924660682678]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_lr.evaluate(test_images, test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hM_SDKpFBpGs"
      },
      "source": [
        "## Train a basic neural network (first try)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_QAMuu0FNxM"
      },
      "source": [
        "The first model network layer transform the 300 by 300 image into single colum,\n",
        "After that we have two layers of activation relu function- because the constant gradient of ReLUs results in faster learning.\n",
        "Finally ,the output layer going to be the same size as the number of labels we trying to classify- we use softmax because it efficient in classification problems. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pULotR1OPZX",
        "outputId": "a546d832-7612-4018-bf21-b371f6586cef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "79/79 [==============================] - 5s 35ms/step - loss: 22.9600 - accuracy: 0.4103\n",
            "Epoch 2/5\n",
            "79/79 [==============================] - 3s 41ms/step - loss: 1.9036 - accuracy: 0.6675\n",
            "Epoch 3/5\n",
            "79/79 [==============================] - 3s 34ms/step - loss: 0.7917 - accuracy: 0.7877\n",
            "Epoch 4/5\n",
            "79/79 [==============================] - 3s 35ms/step - loss: 0.6833 - accuracy: 0.7889\n",
            "Epoch 5/5\n",
            "79/79 [==============================] - 3s 35ms/step - loss: 0.2984 - accuracy: 0.8944\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7efd270c67d0>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = keras.Sequential([\n",
        "  keras.layers.Flatten(),\n",
        "  Dense(512, activation='relu'),\n",
        "  Dense(256, activation='relu'),\n",
        "  Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "#setup loss function\n",
        "model.compile(optimizer='adam',\n",
        "              loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#fit our data to the model\n",
        "model.fit(train_images, train_labels, epochs=5, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkA2gAtmO88d",
        "outputId": "1ae86b45-ebad-4b71-8c74-12cd0bb953da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12/12 [==============================] - 1s 13ms/step - loss: 2.5974 - accuracy: 0.4435\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[2.5974009037017822, 0.44354838132858276]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.evaluate(test_images, test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8R8uA0KfIaAs"
      },
      "source": [
        "In this case we overfitting to our data - the model not learning the train data. \n",
        "we can see it by the results- the accuracy in the train data is 0.89 and tne accu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twfV36ChP4Ce"
      },
      "source": [
        "## Train a Network (convolutional approach)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2cAP7RCfsvP"
      },
      "outputs": [],
      "source": [
        "model.evaluate(test_images, test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3J6Q2DJ6hX7c"
      },
      "source": [
        "## A Better Convolutional Network\n",
        "This time the first layer will be Conv2D()  because the dataset consist of 2D images. The first variable inserted in the function is basically how many times a smaller gird is passing over the image <br>\n",
        "this is how big or smaller grid is so if I said three and I didn't pass in three to start off and we'll leave the rides at one two one that just means they'll move one every time so it's gonna be a sliding window of three by three "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDAtTVtahbIj"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential([\n",
        "   keras.layers.AveragePooling2D(6,3, input_shape=(300,300,1)),\n",
        "   keras.layers.Conv2D(64, 3, activation='relu'),\n",
        "   keras.layers.Conv2D(32, 3, activation='relu'),\n",
        "   keras.layers.MaxPool2D(2,2),\n",
        "   keras.layers.Dropout(0.5),\n",
        "   keras.layers.Flatten(),\n",
        "   keras.layers.Dense(128, activation='relu'),\n",
        "   keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_images, train_labels, epochs=5, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjTdABs_ibEO"
      },
      "outputs": [],
      "source": [
        "model.evaluate(test_images,test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_tDBoCCUTZS"
      },
      "source": [
        "## Plot Image from Numpy Array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7xk18u_UZN7"
      },
      "outputs": [],
      "source": [
        "rgb_images = np.array([example['image'].numpy() for example in ds_train.take(1)])\n",
        "rgb_image = rgb_images[0]\n",
        "plt.imshow(rgb_image)\n",
        "rgb_image.shape\n",
        "\n",
        "#image = train_images[0].reshape(300,300)\n",
        "#plt.imshow(image)\n",
        "\n",
        "# green_images = np.array([example['image'].numpy()[:,:,1] for example in ds_train.take(1)])\n",
        "# green_image = green_images[0].reshape(300,300)\n",
        "# plt.imshow(green_image, cmap='Greys_r')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zUOTuQLOhUN"
      },
      "source": [
        "## Use Model to Predict Result for Single Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51RnZwR5WoBc"
      },
      "outputs": [],
      "source": [
        "result = best_model.predict(np.array([train_images[0]]))\n",
        "print(result)\n",
        "\n",
        "predicted_value = np.argmax(result)\n",
        "print(predicted_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEmWzmWZUgrr"
      },
      "source": [
        "## Convert PNG/JPG images to Numpy Format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JS13XdUOVRbE"
      },
      "outputs": [],
      "source": [
        "import imageio\n",
        "\n",
        "im = imageio.imread('https://upload.wikimedia.org/wikipedia/commons/thumb/8/84/Massachusetts_State_House_-_panoramio_%281%29.jpg/280px-Massachusetts_State_House_-_panoramio_%281%29.jpg')\n",
        "\n",
        "print(type(im))\n",
        "\n",
        "im_np = np.asarray(im)\n",
        "\n",
        "print(im_np.shape)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Rock Paper Scissors Image Classification  ",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
